{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a66d9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BeautifulSoup and splinter\n",
    "from bs4 import BeautifulSoup as soup\n",
    "# from bs4 import SoupStrainer\n",
    "from splinter import Browser\n",
    "\n",
    "# Import other dependencies\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Import geoapify api key\n",
    "from config import geoapify_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfeaa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geoapify url endpoint\n",
    "base_url = \"https://api.geoapify.com/v1/geocode/search\"\n",
    "\n",
    "# List of urls and category used for scraping\n",
    "url_list = []\n",
    "category_list = []\n",
    "\n",
    "url_list.append('https://www.tripadvisor.com/TravelersChoice-Beaches')\n",
    "category_list.append('beaches')\n",
    "\n",
    "url_list.append(\"https://www.tripadvisor.com/TravelersChoice-ThingsToDo\")\n",
    "category_list.append(\"things\")\n",
    "\n",
    "# url_list.append(\"https://www.tripadvisor.com/TravelersChoice-Restaurants\")\n",
    "# category_list.append(\"restaurants\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop thru url list that we need to scrape and get data (done only if csv of data is not present)\n",
    "\n",
    "for index, item in enumerate(url_list):\n",
    "    \n",
    "    search_file = \"static/data/\" + category_list[index] + \".csv\"\n",
    "    csv_file = Path(search_file)\n",
    "\n",
    "    if csv_file.exists():\n",
    "        print(f\"File {csv_file} already extracted. Skipping scrapping\")\n",
    "    else:\n",
    "                \n",
    "        # Set up Splinter\n",
    "        browser = Browser('chrome')\n",
    "\n",
    "        # Visit the website - sleep provided to avoid continuous calls   \n",
    "        browser.visit(item)\n",
    "        time.sleep(30)       \n",
    "        # Optional delay for loading the page\n",
    "        browser.is_element_present_by_css('div.list_text', wait_time=1)\n",
    "\n",
    "        # Scrape the website html\n",
    "        html = browser.html\n",
    "\n",
    "        # Create a BeautifulSoup object from the scraped HTML\n",
    "        data = soup(html, 'html.parser')\n",
    "\n",
    "        # Get Rank and name\n",
    "        names = data.find_all(class_=\"mainName extra\")\n",
    "        namelist = []\n",
    "        ranklist = []\n",
    "        parts = []\n",
    "        for i in names:\n",
    "            href=i.find(\"a\")    \n",
    "            text_data=href.text   \n",
    "            parts = text_data.split('.')    \n",
    "            ranklist.append(parts[0])\n",
    "            namelist.append(parts[1].strip())\n",
    "    \n",
    "        # Get City and Country (single string)\n",
    "        cities = data.find_all(class_=\"smaller\")\n",
    "        citylist = []\n",
    "        lat = []\n",
    "        lon = []\n",
    "        for i in cities:\n",
    "            href=i.find(\"a\")\n",
    "            city_loc = href.text    \n",
    "            citylist.append(city_loc)\n",
    "            \n",
    "            # call geoapify to get lat/lon for location\n",
    "            params = {\n",
    "                \"text\": city_loc,\n",
    "                \"apiKey\": geoapify_key }\n",
    "            # Run request\n",
    "            response = requests.get(base_url, params=params).json()\n",
    "            \n",
    "            # Extract lat/lon\n",
    "            latitude = response[\"features\"][0][\"properties\"][\"lat\"]\n",
    "            longitude = response[\"features\"][0][\"properties\"][\"lon\"]\n",
    "\n",
    "            # append to list\n",
    "            lat.append(latitude)\n",
    "            lon.append(longitude)\n",
    "    \n",
    "\n",
    "        # Get image urls\n",
    "        images = data.find_all(class_=\"sizedThumb_container\")\n",
    "        iurls = []\n",
    "        for i in images:\n",
    "            href=i.find(\"img\")\n",
    "            iurls.append(href[\"src\"])\n",
    "\n",
    "        # Get description of destination/a customer review \n",
    "        desclist = []\n",
    "        for texts in data.find_all(class_=\"quot\"):    \n",
    "            x = texts.find(\"i\").next_sibling.strip()        \n",
    "            desclist.append(x)\n",
    "\n",
    "        # Get url to go retrieve rating and reviews - this will be used later to get details\n",
    "        url_ary = []\n",
    "        for lnk in data.find_all(class_=\"firstone\"):\n",
    "            href=lnk.find(\"a\")    \n",
    "            url_ary.append(href['href'])\n",
    "\n",
    "        # Close browser\n",
    "        browser.quit() \n",
    "\n",
    "        # Create array with category and count of rank array\n",
    "        cat_list = np.repeat([category_list[index]], len(ranklist))\n",
    "\n",
    "        # Load arrays as columns of dataframe\n",
    "        df = pd.DataFrame({\"category\": cat_list, \"rank\": ranklist, \"name\": namelist, \"location\": citylist, \n",
    "                       \"imageurl\": iurls, \"description\": desclist, \"latitude\": lat, \"longitude\": lon, \"ratingurl\": url_ary})\n",
    "        \n",
    "        # save dataframe as csv \n",
    "        filename = \"static/data/\" + category_list[index] + \".csv\"\n",
    "        df.to_csv(filename, encoding=\"utf-8\", index=False, header=True)\n",
    "        \n",
    "\n",
    "    # sleep for a minute before calling next url (providing break to avoid continuous pings to website)\n",
    "    time.sleep(10)\n",
    "\n",
    "# Final checks\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa51ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rating and misc info for all restaurants\n",
    "# prefix = \"https://www.tripadvisor.com\"\n",
    "# url = prefix + \"/Restaurant_Review-g186319-d1209702-Reviews-The_Old_Stamp_House_Restaurant-Ambleside_Lake_District_Cumbria_England.html\"\n",
    "url = \"https://www.tripadvisor.com/Attraction_Review-g10006284-d148331-Reviews-Grace_Bay_Beach-Grace_Bay_Providenciales_Turks_and_Caicos.html\"\n",
    "\n",
    "# setup Splinter\n",
    "browser = Browser('chrome')\n",
    "\n",
    "# Visit the website - sleep provided to avoid continuous calls   \n",
    "browser.visit(url)\n",
    "time.sleep(30)\n",
    "\n",
    "# Scrape the website html\n",
    "html = browser.html\n",
    "# specify select section\n",
    "# parse_section = SoupStrainer(id=['component_46'])\n",
    "# parse_section = SoupStrainer('div', attrs={'class_': 'SrqKb'})\n",
    "\n",
    "# Create a BeautifulSoup object from the scraped HTML\n",
    "# raters = soup(html, 'html.parser', parse_only=parse_section)\n",
    "raters = soup(html, 'html.parser')\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce54b35b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26888\\4089095022.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get overall rating (has decimals)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrating_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class_'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"ZDEqb\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mrating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrating_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# get review count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# get overall rating (has decimals)\n",
    "rating_text = raters.find('span', {'class_': \"ZDEqb\"}).text\n",
    "rating = float(rating_text)\n",
    "\n",
    "# get review count\n",
    "rcount_text = raters.find('a', {'class_': 'IcelI'}).text\n",
    "# remove all ',' separators and cast as int\n",
    "rcount_text = rcount_text.replace(' reviews', '')\n",
    "rcount = int(rcount_text.replace(',',''))  #also remove ' reviews'\n",
    "\n",
    "# get the different types of ratings and their values (repeating 4 times to get 4 rating types)\n",
    "rtype = []\n",
    "rvalue = []\n",
    "div1 = raters.find('div', {'class_': 'DzMcu'})\n",
    "rtype.append(div1.find_next('span', {'class_': 'BPsyj'})).text\n",
    "span1 = div1.find_next('span', {'class_': 'vzATR'})\n",
    "t1 = span1.find_next('span')\n",
    "tx = t1['data-fmid']\n",
    "n1 = float(tx[-2:])\n",
    "n1 = n1/10\n",
    "rvalue.append(n1)\n",
    "\n",
    "div2 = div1.find_next('div', {'class_': 'DzMcu'})\n",
    "rtype.append(div2.find_next('span', {'class_': 'BPsyj'})).text\n",
    "span2 = div2.find_next('span', {'class_': 'vzATR'})\n",
    "t1 = span2.find_next('span')\n",
    "tx = t1['data-fmid']\n",
    "n1 = float(tx[-2:])\n",
    "n1 = n1/10\n",
    "rvalue.append(n1)\n",
    "\n",
    "div3 = div2.find_next('div', {'class_': 'DzMcu'})\n",
    "rtype.append(div3.find_next('span', {'class_': 'BPsyj'})).text\n",
    "span3 = div3.find_next('span', {'class_': 'vzATR'})\n",
    "t1 = span3.find_next('span')\n",
    "tx = t1['data-fmid']\n",
    "n1 = float(tx[-2:])\n",
    "n1 = n1/10\n",
    "rvalue.append(n1)\n",
    "\n",
    "div4 = div3.find('div', {'class_': 'DzMcu'})\n",
    "rtype.append(div4.find_next('span', {'class_': 'BPsyj'})).text\n",
    "span4 = div4.find_next('span', {'class_': 'vzATR'})\n",
    "t1 = span4.find_next('span')\n",
    "tx = t1['data-fmid']\n",
    "n1 = float(tx[-2:])\n",
    "n1 = n1/10\n",
    "rvalue.append(n1)\n",
    "\n",
    "# get price range\n",
    "prange = raters.find('div', {'class_': 'SrqKb'}).text\n",
    "#get cuisines\n",
    "cuisines = raters.find('div', {'class_': 'SrqKb'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "026782e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('htmldat.txt', 'w') as f:\n",
    "    f.write(str(raters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53ca9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('htmldat.txt') as f:\n",
    "    s1 = soup(f, 'html.parser')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fea67545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x1 = s1.find('span', {'class': 'row_num  is-shown-at-tablet'})\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac92a800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "w = raters.find('div', {'class_': 'biGQs _P fiohW hzzSG uuBRH'})\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66477e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb33c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "65d580feb761fe2ba8e44a47e5fcfa7ab48146f0cbe36200890e98e34de43db0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

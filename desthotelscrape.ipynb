{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BeautifulSoup and splinter\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from splinter import Browser\n",
    "\n",
    "# Import other dependencies\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Import geoapify api key\n",
    "geoapify_key = \"f76b71d9aa0c4c4d8e5b01cff5e06c63\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8ea2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Splinter\n",
    "browser = Browser('chrome')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accadb89",
   "metadata": {},
   "source": [
    "### Scraping Top 25 for Hotels and Destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfeaa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geoapify url endpoint\n",
    "base_url = \"https://api.geoapify.com/v1/geocode/search\"\n",
    "\n",
    "# List of urls and category used for scraping\n",
    "url_list = []\n",
    "category_list = []\n",
    "\n",
    "url_list.append('https://www.tripadvisor.com/TravelersChoice-Hotels')\n",
    "category_list.append('hotels')\n",
    "\n",
    "url_list.append(\"https://www.tripadvisor.com/TravelersChoice-Destinations\")\n",
    "category_list.append(\"destinations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop thru url list that we need to scrape and get data (done only if csv of data is not present)\n",
    "\n",
    "for index, item in enumerate(url_list):\n",
    "    \n",
    "    search_file = \"static/data/\" + category_list[index] + \".csv\"\n",
    "    csv_file = Path(search_file)\n",
    "\n",
    "    if csv_file.exists():\n",
    "        print(f\"File {csv_file} already extracted. Skipping scrapping\")\n",
    "    else:\n",
    "                \n",
    "        # Set up Splinter\n",
    "        browser = Browser('chrome')\n",
    "\n",
    "        # Visit the website - sleep provided to avoid continuous calls   \n",
    "        browser.visit(item)\n",
    "        time.sleep(30)       \n",
    "        # Optional delay for loading the page\n",
    "        browser.is_element_present_by_css('div.list_text', wait_time=1)\n",
    "\n",
    "        # Scrape the website html\n",
    "        html = browser.html\n",
    "\n",
    "        # Create a BeautifulSoup object from the scraped HTML\n",
    "        data = soup(html, 'html.parser')\n",
    "\n",
    "        # Get Rank and name\n",
    "        names = data.find_all(class_=\"mainName extra\")\n",
    "        namelist = []\n",
    "        ranklist = []\n",
    "        parts = []\n",
    "        for i in names:\n",
    "            href=i.find(\"a\")    \n",
    "            text_data=href.text   \n",
    "            parts = text_data.split('.')    \n",
    "            ranklist.append(parts[0])\n",
    "            namelist.append(parts[1].strip())\n",
    "    \n",
    "        # Get City and Country (single string)\n",
    "        cities = data.find_all(class_=\"smaller\")\n",
    "        citylist = []\n",
    "        lat = []\n",
    "        lon = []\n",
    "        for i in cities:\n",
    "            href=i.find(\"a\")\n",
    "            city_loc = href.text    \n",
    "            citylist.append(city_loc)\n",
    "            \n",
    "            # call geoapify to get lat/lon for location\n",
    "            params = {\n",
    "                \"text\": city_loc,\n",
    "                \"apiKey\": geoapify_key }\n",
    "            # Run request\n",
    "            response = requests.get(base_url, params=params).json()\n",
    "            \n",
    "            # Extract lat/lon\n",
    "            latitude = response[\"features\"][0][\"properties\"][\"lat\"]\n",
    "            longitude = response[\"features\"][0][\"properties\"][\"lon\"]\n",
    "\n",
    "            # append to list\n",
    "            lat.append(latitude)\n",
    "            lon.append(longitude)\n",
    "    \n",
    "\n",
    "        # Get image urls\n",
    "        images = data.find_all(class_=\"sizedThumb_container\")\n",
    "        iurls = []\n",
    "        for i in images:\n",
    "            href=i.find(\"img\")\n",
    "            iurls.append(href[\"src\"])\n",
    "\n",
    "        # Get description of restaurant/a customer review \n",
    "        desclist = []\n",
    "        quot_tags = data.find_all(class_=\"quot\")\n",
    "        # removing classes with \"quot quot2\" used for second quote as we need just 1 quote per target\n",
    "        quot_tags = [tag for tag in quot_tags if 'quot2' not in ''.join(tag['class'])]\n",
    "        for texts in quot_tags:\n",
    "            x = texts.find(\"i\").next_sibling.strip()        \n",
    "            desclist.append(x)\n",
    "\n",
    "        # Get url to go retrieve rating and reviews - this will be used later to get details\n",
    "        url_ary = []\n",
    "        for lnk in data.find_all(class_=\"firstone\"):\n",
    "            href=lnk.find(\"a\")    \n",
    "            url_ary.append(href['href'])\n",
    "\n",
    "        # Close browser\n",
    "        browser.quit() \n",
    "\n",
    "        # Create array with category and count of rank array\n",
    "        cat_list = np.repeat([category_list[index]], len(ranklist))\n",
    "\n",
    "        # Load arrays as columns of dataframe\n",
    "        df = pd.DataFrame({\"category\": cat_list, \"rank\": ranklist, \"name\": namelist, \"location\": citylist, \n",
    "                       \"imageurl\": iurls, \"description\": desclist, \"latitude\": lat, \"longitude\": lon, \"ratingurl\": url_ary})\n",
    "        \n",
    "        # save dataframe as csv \n",
    "        filename = \"static/data/\" + category_list[index] + \".csv\"\n",
    "        df.to_csv(filename, encoding=\"utf-8\", index=False, header=True)\n",
    "        \n",
    "\n",
    "    # sleep for a minute before calling next url (providing break to avoid continuous pings to website)\n",
    "    time.sleep(10)\n",
    "\n",
    "# Final checks\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45544023",
   "metadata": {},
   "source": [
    "### Scraping Reviews for Hotels Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = []\n",
    "rvcount = []\n",
    "exclnt = []\n",
    "vgood = []\n",
    "averg = []\n",
    "poor = []\n",
    "trrbl = []\n",
    "rank_key = []\n",
    "\n",
    "# Get rating and misc info for all hotels\n",
    "prefix = \"https://www.tripadvisor.com\"\n",
    "\n",
    "hotels_df = pd.read_csv(\"static/data/hotels.csv\")\n",
    "# loop thru hotels and scrape rating information and store as csv\n",
    "for index, row in hotels_df.iterrows():\n",
    "    rank_key.append(row['rank'])\n",
    "    addlink = row['ratingurl']\n",
    "    url = prefix + addlink\n",
    "\n",
    "    # setup Splinter\n",
    "    browser = Browser('chrome')\n",
    "\n",
    "    # Visit the website - sleep provided to avoid continuous calls   \n",
    "    browser.visit(url)\n",
    "    # Scrape the website html\n",
    "    html = browser.html\n",
    "\n",
    "    # Create a BeautifulSoup object from the scraped HTML\n",
    "    raters = soup(html, 'html.parser')\n",
    "    browser.quit()\n",
    "    time.sleep(15)\n",
    "\n",
    "    # extract data and populate arrays\n",
    "    # get overall rating (has decimals)\n",
    "    rating_text = raters.find(class_=\"biGQs _P fiohW hzzSG uuBRH\").text\n",
    "    rating = float(rating_text)\n",
    "    rates.append(rating)\n",
    "\n",
    "    # get review count\n",
    "    rcount_text = raters.find(class_=\"yyzcQ\").text\n",
    "    # remove all ',' separators and cast as int\n",
    "    rcount_text = rcount_text.replace(',', '')\n",
    "    rvcount.append(int(rcount_text))  \n",
    "\n",
    "    # get the different types of ratings and their values\n",
    "    v1 = raters.find_all(class_=\"IMmqe\")\n",
    "    for i in v1:\n",
    "        rv = i.find(class_=\"biGQs _P pZUbB osNWb\").text\n",
    "        rv = rv.replace(',','') \n",
    "        rt = i.find(class_=\"biGQs _P pZUbB hmDzD\").text\n",
    "\n",
    "        if rt == \"Excellent\":\n",
    "            exclnt.append(rv)\n",
    "        elif rt == 'Very good':\n",
    "            vgood.append(rv)\n",
    "        elif rt == 'Average':\n",
    "            averg.append(rv)\n",
    "        elif rt == 'Poor':\n",
    "            poor.append(rv)\n",
    "        elif rt == 'Terrible':\n",
    "            trrbl.append(rv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a1641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to dataframe and save as csv\n",
    "catlist = np.repeat([\"hotels\"], len(rank_key))\n",
    "df1 = pd.DataFrame({\"category\": catlist, \"rank\": rank_key, \"rate\": rates, \"total_reviews\": rvcount, \"excellent\": exclnt, \n",
    "                    \"very_good\": vgood, \"average\": averg, \"poor\": poor, \"terrible\": trrbl})\n",
    "# save dataframe as csv \n",
    "filename = \"static/data/hotelsreviews.csv\"\n",
    "df1.to_csv(filename, encoding=\"utf-8\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "65d580feb761fe2ba8e44a47e5fcfa7ab48146f0cbe36200890e98e34de43db0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
